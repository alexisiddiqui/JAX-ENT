"""
This script performs the fitting of the IsoValidation experiment.

This uses the features generated by the 'featurise_ISO_TRI_BI.py' script.
This then loads the data splits generated by the 'splitdata_ISO.py' script.


This script is used to fit:
- ISO-Bi modal and ISO-Tri modal ensenbles using just the hdx_uptake_mean_centred_MSE_loss.
- The fitting is performed over converagence rates 1e-3 to 1e-10 this is performed using the optimise_sweep function.
The data are saved using hdf5 format.



"""

import copy
import os
import time
from typing import List, Optional, Sequence, Tuple, cast

import jax
import jax.numpy as jnp
import numpy as np
import pandas as pd
from MDAnalysis import Universe
from scipy.spatial.distance import pdist, squareform

import jaxent.src.interfaces.topology as pt
from jaxent.src.custom_types.features import Output_Features
from jaxent.src.custom_types.HDX import HDX_peptide
from jaxent.src.data.loader import ExpD_Dataloader
from jaxent.src.interfaces.model import Model_Parameters
from jaxent.src.interfaces.simulation import Simulation_Parameters
from jaxent.src.models.config import BV_model_Config
from jaxent.src.models.core import Simulation
from jaxent.src.models.HDX.BV.features import BV_input_features
from jaxent.src.models.HDX.BV.forwardmodel import BV_model, BV_Model_Parameters
from jaxent.src.opt.base import InitialisedSimulation, JaxEnt_Loss
from jaxent.src.opt.losses import (
    hdx_uptake_mean_centred_MSE_loss,
    hdx_uptake_MSE_loss,
    maxent_convexKL_loss,
)
from jaxent.src.opt.optimiser import OptaxOptimizer, OptimizationState
from jaxent.src.utils.hdf import (
    save_optimization_history_to_file,
)
from jaxent.src.utils.jit_fn import jit_Guard


def load_hdx_peptides(directory: str, dataset_name: str = "full_dataset") -> List[HDX_peptide]:
    """
    Load HDX_peptide objects from a directory containing topology JSON and dfrac CSV files.

    Args:
        directory: Directory containing the JSON and CSV files
        dataset_name: Base name of the files (e.g., 'train', 'val', 'full_dataset')

    Returns:
        List of HDX_peptide objects

    Raises:
        FileNotFoundError: If required files are not found
        ValueError: If topology and dfrac data have mismatched lengths
    """
    # Construct file paths
    topology_file = os.path.join(directory, f"{dataset_name}_topology.json")
    dfrac_file = os.path.join(directory, f"{dataset_name}_dfrac.csv")

    # Check if files exist
    if not os.path.exists(topology_file):
        raise FileNotFoundError(f"Topology file not found: {topology_file}")
    if not os.path.exists(dfrac_file):
        raise FileNotFoundError(f"Dfrac file not found: {dfrac_file}")

    # Load topology data
    topologies: List[pt.Partial_Topology] = pt.PTSerialiser.load_list_from_json(topology_file)

    # Load dfrac data
    dfrac_df = pd.read_csv(dfrac_file, header=None)
    dfrac_array: np.ndarray = dfrac_df.to_numpy()

    # Validate data consistency
    if len(topologies) != len(dfrac_array):
        raise ValueError(
            f"Mismatch between topology count ({len(topologies)}) "
            f"and dfrac data count ({len(dfrac_array)})"
        )

    # Create HDX_peptide objects
    hdx_peptides: List[HDX_peptide] = [
        HDX_peptide(dfrac=dfrac_array[i].tolist(), top=topologies[i])
        for i in range(len(topologies))
    ]

    print(f"Loaded {len(hdx_peptides)} HDX_peptide objects from {directory}")
    print(f"  Topology: {topology_file}")
    print(f"  Dfrac: {dfrac_file} (shape: {dfrac_array.shape})")

    return hdx_peptides


def load_BV_features(
    feature_path: str,
    feature_top_path: Optional[str] = None,
) -> tuple[BV_input_features, list[pt.Partial_Topology]]:
    """
    Load BV input features from a file.

    Args:
        feature_path: Path to the features file.
        feature_top_path: Path to the topology features file.

    Returns:
        Tuple containing BV_input_features and list of Partial_Topology objects.
    """
    if feature_top_path is None:
        feature_top_path = feature_path.replace(".npz", ".json").replace("features_", "topology_")
    if not os.path.exists(feature_path):
        raise FileNotFoundError(f"Features file not found: {feature_path}")
    if not os.path.exists(feature_top_path):
        raise FileNotFoundError(f"Topology features file not found: {feature_top_path}")

    # Load features
    jnp_features = np.load(feature_path, allow_pickle=True)

    feat_top = pt.PTSerialiser.load_list_from_json(feature_top_path)

    features = BV_input_features(
        heavy_contacts=jnp_features["heavy_contacts"],
        acceptor_contacts=jnp_features["acceptor_contacts"],
        k_ints=jnp_features["k_ints"],
    )

    return (features, feat_top)


def create_data_loaders(
    hdx_data: List[HDX_peptide],
    train_data: List[HDX_peptide],
    val_data: List[HDX_peptide],
    features: BV_input_features,
    feature_top: list[pt.Partial_Topology],
) -> ExpD_Dataloader:
    """
    Create data loaders for training and validation datasets.

    Args:
        train_data: List of HDX_peptide objects for training.
        val_data: List of HDX_peptide objects for validation.
        features: BV_input_features object containing input features.
        feature_top: List of Partial_Topology objects for topology features.

    Returns:
        ExpD_Dataloader object containing the data loaders.
    """

    loader = ExpD_Dataloader(data=hdx_data)
    loader.create_datasets(
        train_data=train_data,
        val_data=val_data,
        features=features,
        feature_topology=feature_top,
    )
    return loader


def optimise_sweep(
    _simulation: InitialisedSimulation,
    data_to_fit: Sequence[ExpD_Dataloader | Model_Parameters | Output_Features],
    n_steps: int,
    tolerance: float,
    convergence: list[float],  # List of convergence thresholds to iterate through
    indexes: Sequence[int],
    loss_functions: Sequence[JaxEnt_Loss],
    opt_state: OptimizationState,
    optimizer: OptaxOptimizer,
) -> Tuple[InitialisedSimulation, OptaxOptimizer]:
    """
    Optimisation method that sequentially iterates through convergence thresholds.
    Only advances to the next threshold after the current one is met.
    Stops only when all thresholds have been satisfied.

    Args:
        convergence: List of convergence thresholds to iterate through sequentially.
    """
    # Sort thresholds from largest to smallest (coarse to fine convergence)
    convergence_thresholds = sorted(convergence, reverse=True)
    current_threshold_idx = 0
    current_threshold = convergence_thresholds[current_threshold_idx]

    print(
        f"Starting optimization with {len(convergence_thresholds)} convergence thresholds: {convergence_thresholds}"
    )
    print(f"Current threshold: {current_threshold}")
    _history = copy.deepcopy(optimizer.history)  # Create a copy of the optimizer's history
    for step in range(n_steps):
        history = optimizer.history
        opt_state, current_loss, history = optimizer.step(
            optimizer=optimizer,
            state=opt_state,
            simulation=_simulation,
            data_targets=tuple(data_to_fit),
            loss_functions=tuple(loss_functions),
            indexes=tuple(indexes),
            history=history,
        )

        # Print progress every step
        jax.debug.print(
            fmt=" ".join(
                [
                    "Step {step}/{n_steps}",
                    "Training Loss: {train_loss:.2f}",
                    "Validation Loss: {val_loss:.2f}",
                    "Threshold {threshold_idx}/{total_thresholds} ({current_threshold:.6f})",
                ]
            ),
            step=step,
            n_steps=n_steps,
            train_loss=opt_state.losses.total_train_loss,
            val_loss=opt_state.losses.total_val_loss,
            threshold_idx=current_threshold_idx + 1,
            total_thresholds=len(convergence_thresholds),
            current_threshold=current_threshold,
        )

        # Check for tolerance, nan, or inf conditions (early termination)
        if (current_loss < tolerance) or (current_loss == jnp.nan) or (current_loss == jnp.inf):
            print(f"Reached convergence tolerance/nan vals at step {step}, loss: {current_loss}")
            break
        if step == 0:
            _history.add_state(opt_state)

        # Check convergence for current threshold
        if step > 10:
            previous_loss = optimizer.history.states[-2].losses.total_train_loss
            loss_delta = abs(current_loss - previous_loss)

            # Check if current threshold is met
            if loss_delta < current_threshold:
                print(
                    f"Threshold {current_threshold_idx + 1}/{len(convergence_thresholds)} met at step {step}"
                )
                print(f"Loss delta: {loss_delta:.6f}, threshold: {current_threshold}")
                _history.add_state(opt_state)
                print(f"History updated with state at step {step}, loss: {current_loss:.6f}")

                # Move to next threshold
                current_threshold_idx += 1

                # Check if all thresholds have been satisfied
                if current_threshold_idx >= len(convergence_thresholds):
                    print(f"All convergence thresholds completed at step {step}")
                    break
                else:
                    # Update to next threshold
                    current_threshold = convergence_thresholds[current_threshold_idx]
                    print(
                        f"Moving to threshold {current_threshold_idx + 1}/{len(convergence_thresholds)}: {current_threshold}"
                    )
    optimizer.history = _history  # Update the optimizer's history with the copied history
    # Get best state from recorded history
    best_state = optimizer.history.get_best_state()
    if best_state is not None:
        _simulation.params = optimizer.history.best_state.params

    _simulation = cast(InitialisedSimulation, _simulation)
    return _simulation, optimizer


@jit_Guard.clear_caches_after()
def run_optimise_ISO_TRI_BI(
    train_data: List[HDX_peptide],
    val_data: List[HDX_peptide],
    features: BV_input_features,
    forward_model: BV_model,
    model_parameters: BV_Model_Parameters,
    feature_top: List[pt.Partial_Topology],
    convergence: List[float],
    loss_function: JaxEnt_Loss,
    pairwise_similarity: jax.Array,
    n_steps: int = 10,
    name: str = "ISO_TRI_BI",
    output_dir: str = "_optimise",
) -> None:
    # create dataloader
    data_to_fit = create_data_loaders(
        hdx_data=train_data + val_data,
        train_data=train_data,
        val_data=val_data,
        features=features,
        feature_top=feature_top,
    )

    n_frames = features.features_shape[1]  # Assuming features.features_shape (n_residues, n_frames)

    parameters = Simulation_Parameters(
        frame_weights=jnp.ones(n_frames) / n_frames,
        frame_mask=jnp.ones(n_frames),
        model_parameters=(model_parameters,),
        forward_model_weights=jnp.array([1.0, 0.1]),  # Added weight for consistency loss
        normalise_loss_functions=jnp.ones(2),  # Adjusted for 3 loss functions
        forward_model_scaling=jnp.ones(2),  # Adjusted for 3 loss functions
    )

    # create initialised simulation
    sim = Simulation(input_features=(features,), forward_models=(forward_model,), params=parameters)
    sim.initialise()

    optimizer = OptaxOptimizer(
        learning_rate=1e-3,
        optimizer="adam",
    )
    opt_state = optimizer.initialise(
        model=sim,
        optimisable_funcs=None,
    )
    _, optimizer = optimise_sweep(
        _simulation=sim,
        data_to_fit=(data_to_fit, parameters),  # Added pairwise_similarity
        n_steps=n_steps,
        tolerance=1e-10,
        convergence=convergence,
        indexes=[0, 0],  # Adjusted for 3 loss functions
        loss_functions=[
            loss_function,
            maxent_convexKL_loss,
        ],  # Added consistency loss
        opt_state=opt_state,
        optimizer=optimizer,
    )

    # Save the results
    output_path = os.path.join(output_dir, f"{name}_results.hdf5")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    save_optimization_history_to_file(filename=output_path, history=optimizer.history)


def main():
    # define parameters
    ensembles = ["ISO_TRI", "ISO_BI"]
    losses = {"mcMSE": hdx_uptake_mean_centred_MSE_loss, "MSE": hdx_uptake_MSE_loss}
    n_steps = 10000  # Increased from 10 for proper optimization
    num_splits = 3

    # Define convergence criteria from 1e-3 to 1e-10 as mentioned in docstring
    convergence_rates = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10]

    datasplit_dir = "_datasplits"
    datasplit_dir = os.path.join(os.path.dirname(__file__), datasplit_dir)
    if not os.path.exists(datasplit_dir):
        raise FileNotFoundError(f"Datasplit directory not found: {datasplit_dir}")

    features_dir = "_featurise"
    features_dir = os.path.join(os.path.dirname(__file__), features_dir)
    if not os.path.exists(features_dir):
        raise FileNotFoundError(f"Features directory not found: {features_dir}")

    # Load features - Fixed the path mapping
    feature_paths = {
        "ISO_TRI": os.path.join(features_dir, "features_iso_tri.npz"),
        "ISO_BI": os.path.join(features_dir, "features_iso_bi.npz"),
    }

    features = {key: load_BV_features(path) for key, path in feature_paths.items()}

    # Setup BV model configuration
    bv_config = BV_model_Config(num_timepoints=5)
    bv_config.timepoints = jnp.array([0.167, 1.0, 10.0, 60.0, 120.0])
    bv_model = BV_model(config=bv_config)
    model_parameters = bv_model.params
    print(bv_model.config.key)  # Print the model key for debugging

    # Create base output directory
    output_base_dir = "_optimise_cKL"
    output_base_dir = os.path.join(os.path.dirname(__file__), output_base_dir)
    os.makedirs(output_base_dir, exist_ok=True)

    # Discover split types
    # Assuming split types are subdirectories within datasplit_dir
    split_types = [
        d for d in os.listdir(datasplit_dir) if os.path.isdir(os.path.join(datasplit_dir, d))
    ]
    if "full_dataset" in split_types:
        split_types.remove("full_dataset")  # Exclude the full_dataset directory from split types
    split_types.sort()  # Ensure consistent order
    # split_types = ["sequence_cluster"] # Uncomment this line if you only want to run for a specific split type

    # Run optimization for all combinations
    total_runs = len(ensembles) * len(losses) * num_splits * len(split_types)
    current_run = 0

    print(f"Starting optimization with {total_runs} total runs...")
    print(f"Split Types: {split_types}")
    print(f"Ensembles: {ensembles}")
    print(f"Loss functions: {list(losses.keys())}")
    print(f"Number of splits per type: {num_splits}")
    print(f"Convergence rates: {convergence_rates}")
    print(f"Max steps per run: {n_steps}")
    print("-" * 60)

    total_start_time = time.time()  # Start total timer

    # Define trajectory and topology paths based on the featurise_ISO_TRI_BI.py script
    # These paths are relative to the JAX-ENT project root or can be made absolute.
    # For consistency, let's construct absolute paths based on the project structure.
    base_traj_dir = os.path.join(
        os.path.dirname(__file__), "../../data/_Bradshaw/Reproducibility_pack_v2/data/trajectories"
    )

    topology_file = os.path.join(base_traj_dir, "TeaA_ref_closed_state.pdb")
    topology_file = os.path.join(base_traj_dir, "TeaA_ref_closed_state.pdb")
    tri_modal_traj_file = os.path.join(
        base_traj_dir,
        "sliced_trajectories/TeaA_filtered_sliced.xtc",
    )
    bi_modal_traj_file = os.path.join(
        base_traj_dir, "sliced_trajectories/TeaA_initial_sliced.xtc"
    )  # Check if the trajectory and topology files exist
    if not os.path.exists(topology_file):
        raise FileNotFoundError(f"Topology file not found: {topology_file}")
    if not os.path.exists(tri_modal_traj_file):
        raise FileNotFoundError(f"Tri-modal trajectory file not found: {tri_modal_traj_file}")
    if not os.path.exists(bi_modal_traj_file):
        raise FileNotFoundError(f"Bi-modal trajectory file not found: {bi_modal_traj_file}")

    for split_type in split_types:
        print(f"\n-- Processing split type: {split_type} --")
        split_type_dir = os.path.join(datasplit_dir, split_type)
        output_dir = os.path.join(output_base_dir, split_type)
        os.makedirs(output_dir, exist_ok=True)

        # Load all data splits for this type
        splits = []
        try:
            for split_idx in range(num_splits):
                split_path = os.path.join(split_type_dir, f"split_{split_idx:03d}")
                train_data = load_hdx_peptides(split_path, dataset_name="train")
                val_data = load_hdx_peptides(split_path, dataset_name="val")
                splits.append((train_data, val_data))
        except FileNotFoundError as e:
            print(f"Could not load splits for {split_type}. Skipping. Error: {e}")
            continue

        for ensemble in ensembles:
            print(f"\nProcessing ensemble: {ensemble}")
            ensemble_features, ensemble_feature_top = features[ensemble]

            # Determine the correct trajectory path for the current ensemble
            current_trajectory_path = ""
            if ensemble == "ISO_TRI":
                current_trajectory_path = tri_modal_traj_file
            elif ensemble == "ISO_BI":
                current_trajectory_path = bi_modal_traj_file
            else:
                raise ValueError(f"Unknown ensemble type: {ensemble}")

            # Calculate pairwise similarity matrix for the current ensemble
            print(f"Calculating pairwise similarity for {ensemble} using {current_trajectory_path}")
            universe = Universe(topology_file, current_trajectory_path)
            ca_atoms = universe.select_atoms("name CA")

            ca_coords_by_frame = []
            for ts in universe.trajectory:
                ca_coords_by_frame.append(pdist(ca_atoms.positions).flatten())

            ca_coords_matrix = np.vstack(ca_coords_by_frame)
            cosine_distances = squareform(pdist(ca_coords_matrix, metric="cosine"))
            pairwise_similarity = jnp.array(cosine_distances)
            print(f"Pairwise similarity matrix shape for {ensemble}: {pairwise_similarity.shape}")

            for loss_name, loss_function in losses.items():
                print(f"  Using loss function: {loss_name}")

                for split_idx, (train_data, val_data) in enumerate(splits):
                    current_run += 1

                    # Create descriptive name for this run
                    run_name = f"{ensemble}_{loss_name}_{split_type}_split{split_idx:03d}"

                    print(f"    Running split {split_idx} [{current_run}/{total_runs}]: {run_name}")
                    print(f"      Train samples: {len(train_data)}, Val samples: {len(val_data)}")

                    run_start_time = time.time()  # Start timer for this run

                    try:
                        run_optimise_ISO_TRI_BI(
                            train_data=train_data,
                            val_data=val_data,
                            features=ensemble_features,
                            forward_model=bv_model,
                            model_parameters=model_parameters,
                            feature_top=ensemble_feature_top,
                            convergence=convergence_rates,
                            loss_function=loss_function,
                            pairwise_similarity=pairwise_similarity,
                            n_steps=n_steps,
                            name=run_name,
                            output_dir=output_dir,
                        )
                        run_elapsed = time.time() - run_start_time
                        print(f"      ✓ Completed: {run_name} (Elapsed: {run_elapsed:.2f} s)")

                    except Exception as e:
                        run_elapsed = time.time() - run_start_time
                        print(
                            f"      ✗ Failed: {run_name} - Error: {str(e)} (Elapsed: {run_elapsed:.2f} s)"
                        )
                        # Continue with other runs even if one fails
                        continue

    total_elapsed = time.time() - total_start_time  # End total timer

    print(f"\n{'=' * 60}")
    print("Optimization completed!")
    print(f"Results saved in: {output_base_dir}")
    print(f"Total runs attempted: {total_runs}")
    print(f"Total elapsed time: {total_elapsed:.2f} s")
    print("Check individual HDF5 files for detailed results.")


if __name__ == "__main__":
    main()
