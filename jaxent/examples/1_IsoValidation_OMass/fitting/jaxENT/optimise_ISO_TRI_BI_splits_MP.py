"""
This script performs the fitting of the IsoValidation experiment with multiprocessing.

This uses the features generated by the 'featurise_ISO_TRI_BI.py' script.
This then loads the data splits generated by the 'splitdata_ISO.py' script.

This script is used to fit:
- ISO-Bi modal and ISO-Tri modal ensembles using different loss functions.
- The fitting is performed over convergence rates 1e-3 to 1e-10 using the optimise_sweep function.
- Ensemble-loss combinations are run in parallel using subprocess.
- Replicates and split types are processed within the same subprocess.

Usage:
python optimise_ISO_TRI_BI_splits_multiprocess.py --split-types all --max-processes 4
python optimise_ISO_TRI_BI_splits_multiprocess.py --split-types random,sequence --max-processes 2
python optimise_ISO_TRI_BI_splits_multiprocess.py --ensemble ISO_TRI --loss-function mcMSE --split-types all  # Single process mode
"""

import argparse
import multiprocessing as mp
import os
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Optional

import jax

os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"

jax.config.update("jax_platform_name", "cpu")
os.environ["JAX_PLATFORM_NAME"] = "cpu"
# Import JAX and model components inside worker functions to avoid CUDA context issues


def get_loss_function(loss_name: str):
    """Get loss function by name."""
    # Import inside function to avoid CUDA context issues
    from jaxent.src.opt.losses import (
        hdx_uptake_mean_centred_MSE_loss,
        hdx_uptake_MSE_loss,
    )

    losses = {"mcMSE": hdx_uptake_mean_centred_MSE_loss, "MSE": hdx_uptake_MSE_loss}
    if loss_name not in losses:
        raise ValueError(f"Unknown loss function: {loss_name}. Available: {list(losses.keys())}")
    return losses[loss_name]


def run_single_combination(
    ensemble: str,
    loss_name: str,
    split_types_arg: str,
    n_steps: int,
    num_splits: int,
    process_id: int = 0,
) -> dict:
    """
    Run optimization for a single ensemble-loss combination across all specified split types and replicates.

    Args:
        ensemble: Ensemble type (ISO_TRI or ISO_BI)
        loss_name: Loss function name (mcMSE or MSE)
        split_types_arg: Comma-separated split types or 'all'
        n_steps: Number of optimization steps
        num_splits: Number of replicates per split type
        process_id: Process identifier for logging

    Returns:
        dict: Results summary for this combination
    """
    # Import JAX components inside function to avoid CUDA context issues
    import jax
    import jax.numpy as jnp
    from optimise_fn import run_optimise_ISO_TRI_BI

    os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"

    jax.config.update("jax_platform_name", "cpu")
    os.environ["JAX_PLATFORM_NAME"] = "cpu"
    import jaxent.src.interfaces.topology as pt
    from jaxent.src.custom_types.HDX import HDX_peptide
    from jaxent.src.models.config import BV_model_Config
    from jaxent.src.models.HDX.BV.features import BV_input_features
    from jaxent.src.models.HDX.BV.forwardmodel import BV_model

    start_time = time.time()
    print(f"Process {process_id}: Starting {ensemble}-{loss_name} combination")

    # Define convergence criteria
    convergence_rates = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10]

    # Setup directories
    datasplit_dir = os.path.join(os.path.dirname(__file__), "_datasplits")
    features_dir = os.path.join(os.path.dirname(__file__), "_featurise")
    output_base_dir = os.path.join(os.path.dirname(__file__), "_optimise")

    if not os.path.exists(datasplit_dir):
        raise FileNotFoundError(f"Datasplit directory not found: {datasplit_dir}")
    if not os.path.exists(features_dir):
        raise FileNotFoundError(f"Features directory not found: {features_dir}")

    os.makedirs(output_base_dir, exist_ok=True)

    # Load features for this ensemble
    feature_path = os.path.join(features_dir, f"features_{ensemble.lower()}.npz")
    topology_path = feature_path.replace("features_", "topology_").replace(".npz", ".json")

    features = BV_input_features.load(feature_path)
    feature_top = pt.PTSerialiser.load_list_from_json(topology_path)

    # Get loss function
    loss_function = get_loss_function(loss_name)

    # Setup BV model
    bv_config = BV_model_Config(num_timepoints=5)
    bv_config.timepoints = jnp.array([0.167, 1.0, 10.0, 60.0, 120.0])
    bv_model = BV_model(config=bv_config)
    model_parameters = bv_model.params

    # Discover split types
    split_types = [
        d
        for d in os.listdir(datasplit_dir)
        if os.path.isdir(os.path.join(datasplit_dir, d)) and d != "full_dataset"
    ]
    split_types.sort()

    # Filter split types if specified
    if split_types_arg != "all":
        requested = [s.strip() for s in split_types_arg.split(",")]
        split_types = [s for s in split_types if s in requested]
        if not split_types:
            raise ValueError(f"No valid split types selected from: {requested}")

    # Track results
    results = {
        "ensemble": ensemble,
        "loss_name": loss_name,
        "process_id": process_id,
        "completed_runs": 0,
        "failed_runs": 0,
        "total_runs": len(split_types) * num_splits,
        "split_types": split_types,
        "start_time": start_time,
    }

    print(
        f"Process {process_id}: Processing {len(split_types)} split types with {num_splits} replicates each"
    )

    # Process all split types and replicates for this ensemble-loss combination
    for split_type in split_types:
        print(f"Process {process_id}: Processing split type: {split_type}")
        split_type_dir = os.path.join(datasplit_dir, split_type)
        output_dir = os.path.join(output_base_dir, split_type)
        os.makedirs(output_dir, exist_ok=True)

        # Load all data splits for this type
        splits = []
        try:
            for split_idx in range(num_splits):
                split_path = os.path.join(split_type_dir, f"split_{split_idx:03d}")

                train_data = HDX_peptide.load_list_from_files(
                    json_path=os.path.join(split_path, "train_topology.json"),
                    csv_path=os.path.join(split_path, "train_dfrac.csv"),
                )
                val_data = HDX_peptide.load_list_from_files(
                    json_path=os.path.join(split_path, "val_topology.json"),
                    csv_path=os.path.join(split_path, "val_dfrac.csv"),
                )
                splits.append((train_data, val_data))
        except FileNotFoundError as e:
            print(
                f"Process {process_id}: Could not load splits for {split_type}. Skipping. Error: {e}"
            )
            continue

        # Process all replicates for this split type
        for split_idx, (train_data, val_data) in enumerate(splits):
            run_name = f"{ensemble}_{loss_name}_{split_type}_split{split_idx:03d}"

            print(f"Process {process_id}: Running {run_name}")
            print(
                f"Process {process_id}: Train samples: {len(train_data)}, Val samples: {len(val_data)}"
            )

            run_start_time = time.time()

            try:
                run_optimise_ISO_TRI_BI(
                    train_data=train_data,
                    val_data=val_data,
                    features=features,
                    forward_model=bv_model,
                    model_parameters=model_parameters,
                    feature_top=feature_top,
                    convergence=convergence_rates,
                    loss_function=loss_function,
                    n_steps=n_steps,
                    name=run_name,
                    output_dir=output_dir,
                )
                run_elapsed = time.time() - run_start_time
                print(
                    f"Process {process_id}: ✓ Completed: {run_name} (Elapsed: {run_elapsed:.2f} s)"
                )
                results["completed_runs"] += 1

            except Exception as e:
                run_elapsed = time.time() - run_start_time
                print(
                    f"Process {process_id}: ✗ Failed: {run_name} - Error: {str(e)} (Elapsed: {run_elapsed:.2f} s)"
                )
                results["failed_runs"] += 1
                continue

    total_elapsed = time.time() - start_time
    results["total_elapsed"] = total_elapsed

    print(f"Process {process_id}: Completed {ensemble}-{loss_name} combination")
    print(
        f"Process {process_id}: {results['completed_runs']}/{results['total_runs']} successful runs"
    )
    print(f"Process {process_id}: Total elapsed time: {total_elapsed:.2f} s")

    return results


def run_multiprocess_optimization(
    split_types_arg: str = "all", n_steps: int = 10000, num_splits: int = 3, max_processes: int = 4
) -> None:
    """
    Run optimization using multiprocessing for ensemble-loss combinations.

    Args:
        split_types_arg: Split types to process ('all' or comma-separated list)
        n_steps: Number of optimization steps per run
        num_splits: Number of replicates per split type
        max_processes: Maximum number of parallel processes
    """
    # Set multiprocessing start method to avoid JAX/CUDA context issues
    try:
        mp.set_start_method("spawn", force=True)
    except RuntimeError:
        pass  # Already set

    ensembles = ["ISO_TRI", "ISO_BI"]
    loss_names = ["mcMSE", "MSE"]

    # Create all ensemble-loss combinations
    combinations = [(ensemble, loss_name) for ensemble in ensembles for loss_name in loss_names]

    print(f"Starting multiprocess optimization with {len(combinations)} combinations")
    print(f"Combinations: {combinations}")
    print(f"Max processes: {max_processes}")
    print(f"Steps per run: {n_steps}")
    print(f"Replicates per split type: {num_splits}")
    print("-" * 60)

    total_start_time = time.time()
    all_results = []

    # Use ProcessPoolExecutor to manage parallel execution
    with ProcessPoolExecutor(max_workers=max_processes) as executor:
        # Submit all combinations
        future_to_combo = {
            executor.submit(
                run_single_combination, ensemble, loss_name, split_types_arg, n_steps, num_splits, i
            ): (ensemble, loss_name)
            for i, (ensemble, loss_name) in enumerate(combinations)
        }

        # Collect results as they complete
        for future in as_completed(future_to_combo):
            ensemble, loss_name = future_to_combo[future]
            try:
                result = future.result()
                all_results.append(result)
                print(f"✓ Completed combination: {ensemble}-{loss_name}")
            except Exception as e:
                print(f"✗ Failed combination: {ensemble}-{loss_name} - Error: {str(e)}")

    total_elapsed = time.time() - total_start_time

    # Print summary
    print(f"\n{'=' * 60}")
    print("Multiprocess optimization completed!")

    total_runs = sum(r["completed_runs"] + r["failed_runs"] for r in all_results)
    successful_runs = sum(r["completed_runs"] for r in all_results)
    failed_runs = sum(r["failed_runs"] for r in all_results)

    print(f"Total combinations processed: {len(all_results)}/{len(combinations)}")
    print(f"Total runs: {total_runs}")
    print(f"Successful runs: {successful_runs}")
    print(f"Failed runs: {failed_runs}")
    print(f"Success rate: {100 * successful_runs / total_runs:.1f}%")
    print(f"Total elapsed time: {total_elapsed:.2f} s")

    # Print per-combination summary
    print("\nPer-combination summary:")
    for result in all_results:
        success_rate = (
            100 * result["completed_runs"] / result["total_runs"] if result["total_runs"] > 0 else 0
        )
        print(
            f"  {result['ensemble']}-{result['loss_name']}: {result['completed_runs']}/{result['total_runs']} runs ({success_rate:.1f}%) in {result['total_elapsed']:.1f}s"
        )

    print(f"\nResults saved in: {os.path.join(os.path.dirname(__file__), '_optimise')}")


def main(
    split_types_arg: Optional[str] = None,
    n_steps: int = 10000,
    num_splits: int = 3,
    max_processes: Optional[int] = None,
    ensemble: Optional[str] = None,
    loss_function: Optional[str] = None,
) -> None:
    """
    Main function that either runs multiprocess optimization or single combination.

    If ensemble and loss_function are specified, runs in single-process mode for that combination.
    Otherwise, runs multiprocess optimization for all combinations.
    """
    # Single process mode (for subprocess calls or targeted runs)
    if ensemble is not None and loss_function is not None:
        print(f"Running single combination: {ensemble}-{loss_function}")
        result = run_single_combination(
            ensemble=ensemble,
            loss_name=loss_function,
            split_types_arg=split_types_arg or "all",
            n_steps=n_steps,
            num_splits=num_splits,
            process_id=0,
        )
        print(
            f"Single combination completed: {result['completed_runs']}/{result['total_runs']} successful runs"
        )
        return

    # Multiprocess mode
    if max_processes is None:
        # Default to number of combinations (4) or 4, whichever is smaller
        max_processes = min(4, 4)  # 2 ensembles * 2 loss functions = 4 combinations

    run_multiprocess_optimization(
        split_types_arg=split_types_arg or "all",
        n_steps=n_steps,
        num_splits=num_splits,
        max_processes=max_processes,
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Optimise ISO_TRI/BI splits with multiprocessing.")
    parser.add_argument(
        "--split-types",
        type=str,
        default="all",
        help="Comma-separated list of split types to run (e.g. 'random,sequence'). Use 'all' for all types.",
    )
    parser.add_argument(
        "--n-steps",
        type=int,
        default=10000,
        help="Number of optimization steps per run (default: 10000).",
    )
    parser.add_argument(
        "--n-replicates",
        type=int,
        default=3,
        help="Number of replicates (splits) per split type (default: 3).",
    )
    parser.add_argument(
        "--max-processes",
        type=int,
        default=4,
        help="Maximum number of parallel processes (default: 4).",
    )
    parser.add_argument(
        "--ensemble",
        type=str,
        choices=["ISO_TRI", "ISO_BI"],
        help="Specific ensemble to run (for single-process mode).",
    )
    parser.add_argument(
        "--loss-function",
        type=str,
        choices=["mcMSE", "MSE"],
        help="Specific loss function to run (for single-process mode).",
    )

    args = parser.parse_args()

    main(
        split_types_arg=args.split_types,
        n_steps=args.n_steps,
        num_splits=args.n_replicates,
        max_processes=args.max_processes,
        ensemble=args.ensemble,
        loss_function=args.loss_function,
    )
