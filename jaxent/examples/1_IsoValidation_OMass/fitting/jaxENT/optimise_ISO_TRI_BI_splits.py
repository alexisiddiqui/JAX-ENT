"""
This script performs the fitting of the IsoValidation experiment.

This uses the features generated by the 'featurise_ISO_TRI_BI.py' script.
This then loads the data splits generated by the 'splitdata_ISO.py' script.


This script is used to fit:
- ISO-Bi modal and ISO-Tri modal ensenbles using just the hdx_uptake_mean_centred_MSE_loss.
- The fitting is performed over converagence rates 1e-3 to 1e-10 this is performed using the optimise_sweep function.
The data are saved using hdf5 format.

Usage:
python optimise_ISO_TRI_BI_splits.py --split-types all
python optimise_ISO_TRI_BI_splits.py --split-types random,sequence,sequence_cluster,stratified,spatial


"""

import argparse  # <-- Add this import
import copy
import os
import time  # <-- Add this import
from typing import List, Sequence, Tuple, cast

import jax
import jax.numpy as jnp

import jaxent.src.interfaces.topology as pt
from jaxent.src.custom_types.features import Output_Features
from jaxent.src.custom_types.HDX import HDX_peptide
from jaxent.src.data.loader import ExpD_Dataloader
from jaxent.src.interfaces.model import Model_Parameters
from jaxent.src.interfaces.simulation import Simulation_Parameters
from jaxent.src.models.config import BV_model_Config
from jaxent.src.models.core import Simulation
from jaxent.src.models.HDX.BV.features import BV_input_features
from jaxent.src.models.HDX.BV.forwardmodel import BV_model, BV_Model_Parameters
from jaxent.src.opt.base import InitialisedSimulation, JaxEnt_Loss
from jaxent.src.opt.losses import (
    hdx_uptake_mean_centred_MSE_loss,
    hdx_uptake_MSE_loss,
)
from jaxent.src.opt.optimiser import OptaxOptimizer, OptimizationState
from jaxent.src.utils.hdf import (
    save_optimization_history_to_file,
)
from jaxent.src.utils.jit_fn import jit_Guard


def create_data_loaders(
    hdx_data: List[HDX_peptide],
    train_data: List[HDX_peptide],
    val_data: List[HDX_peptide],
    features: BV_input_features,
    feature_top: list[pt.Partial_Topology],
) -> ExpD_Dataloader:
    """
    Create data loaders for training and validation datasets.

    Args:
        train_data: List of HDX_peptide objects for training.
        val_data: List of HDX_peptide objects for validation.
        features: BV_input_features object containing input features.
        feature_top: List of Partial_Topology objects for topology features.

    Returns:
        ExpD_Dataloader object containing the data loaders.
    """

    loader = ExpD_Dataloader(data=hdx_data)
    loader.create_datasets(
        train_data=train_data,
        val_data=val_data,
        features=features,
        feature_topology=feature_top,
    )
    return loader


def optimise_sweep(
    _simulation: InitialisedSimulation,
    data_to_fit: Sequence[ExpD_Dataloader | Model_Parameters | Output_Features],
    n_steps: int,
    tolerance: float,
    convergence: list[float],  # List of convergence thresholds to iterate through
    indexes: Sequence[int],
    loss_functions: Sequence[JaxEnt_Loss],
    opt_state: OptimizationState,
    optimizer: OptaxOptimizer,
) -> Tuple[InitialisedSimulation, OptaxOptimizer]:
    """
    Optimisation method that sequentially iterates through convergence thresholds.
    Only advances to the next threshold after the current one is met.
    Stops only when all thresholds have been satisfied.

    Args:
        convergence: List of convergence thresholds to iterate through sequentially.
    """
    # Sort thresholds from largest to smallest (coarse to fine convergence)
    convergence_thresholds = sorted(convergence, reverse=True)
    current_threshold_idx = 0
    current_threshold = convergence_thresholds[current_threshold_idx]

    print(
        f"Starting optimization with {len(convergence_thresholds)} convergence thresholds: {convergence_thresholds}"
    )
    print(f"Current threshold: {current_threshold}")
    _history = copy.deepcopy(optimizer.history)  # Create a copy of the optimizer's history
    for step in range(n_steps):
        history = optimizer.history
        opt_state, current_loss, history = optimizer.step(
            optimizer=optimizer,
            state=opt_state,
            simulation=_simulation,
            data_targets=tuple(data_to_fit),
            loss_functions=tuple(loss_functions),
            indexes=tuple(indexes),
            history=history,
        )

        # Print progress every step
        jax.debug.print(
            fmt=" ".join(
                [
                    "Step {step}/{n_steps}",
                    "Training Loss: {train_loss:.2f}",
                    "Validation Loss: {val_loss:.2f}",
                    "Threshold {threshold_idx}/{total_thresholds} ({current_threshold:.6f})",
                ]
            ),
            step=step,
            n_steps=n_steps,
            train_loss=opt_state.losses.total_train_loss,
            val_loss=opt_state.losses.total_val_loss,
            threshold_idx=current_threshold_idx + 1,
            total_thresholds=len(convergence_thresholds),
            current_threshold=current_threshold,
        )

        # Check for tolerance, nan, or inf conditions (early termination)
        if (current_loss < tolerance) or (current_loss == jnp.nan) or (current_loss == jnp.inf):
            print(f"Reached convergence tolerance/nan vals at step {step}, loss: {current_loss}")
            break
        if step == 0:
            _history.add_state(opt_state)

        # Check convergence for current threshold
        if step > 10:
            previous_loss = optimizer.history.states[-2].losses.total_train_loss
            loss_delta = abs(current_loss - previous_loss)

            # Check if current threshold is met
            if loss_delta < current_threshold:
                print(
                    f"Threshold {current_threshold_idx + 1}/{len(convergence_thresholds)} met at step {step}"
                )
                print(f"Loss delta: {loss_delta:.6f}, threshold: {current_threshold}")
                _history.add_state(opt_state)
                print(f"History updated with state at step {step}, loss: {current_loss:.6f}")

                # Move to next threshold
                current_threshold_idx += 1

                # Check if all thresholds have been satisfied
                if current_threshold_idx >= len(convergence_thresholds):
                    print(f"All convergence thresholds completed at step {step}")
                    break
                else:
                    # Update to next threshold
                    current_threshold = convergence_thresholds[current_threshold_idx]
                    print(
                        f"Moving to threshold {current_threshold_idx + 1}/{len(convergence_thresholds)}: {current_threshold}"
                    )
    optimizer.history = _history  # Update the optimizer's history with the copied history
    # Get best state from recorded history
    best_state = optimizer.history.get_best_state()
    if best_state is not None:
        _simulation.params = optimizer.history.best_state.params

    _simulation = cast(InitialisedSimulation, _simulation)
    return _simulation, optimizer


@jit_Guard.clear_caches_after()
def run_optimise_ISO_TRI_BI(
    train_data: List[HDX_peptide],
    val_data: List[HDX_peptide],
    features: BV_input_features,
    forward_model: BV_model,
    model_parameters: BV_Model_Parameters,
    feature_top: List[pt.Partial_Topology],
    convergence: List[float],
    loss_function: JaxEnt_Loss,
    n_steps: int = 10,
    name: str = "ISO_TRI_BI",
    output_dir: str = "_optimise",
) -> None:
    # create dataloader
    data_to_fit = create_data_loaders(
        hdx_data=train_data + val_data,
        train_data=train_data,
        val_data=val_data,
        features=features,
        feature_top=feature_top,
    )

    n_frames = features.features_shape[1]  # Assuming features.features_shape (n_residues, n_frames)

    parameters = Simulation_Parameters(
        frame_weights=jnp.ones(n_frames) / n_frames,
        frame_mask=jnp.ones(n_frames),
        model_parameters=(model_parameters,),
        forward_model_weights=jnp.array([1.0]),
        normalise_loss_functions=jnp.ones(1),
        forward_model_scaling=jnp.ones(1),
    )

    # create initialised simulation
    sim = Simulation(input_features=(features,), forward_models=(forward_model,), params=parameters)
    sim.initialise()

    optimizer = OptaxOptimizer(
        learning_rate=1e-3,
        optimizer="adam",
    )
    opt_state = optimizer.initialise(
        model=sim,
        optimisable_funcs=None,
    )
    _, optimizer = optimise_sweep(
        _simulation=sim,
        data_to_fit=(data_to_fit,),
        n_steps=n_steps,
        tolerance=1e-10,
        convergence=convergence,
        indexes=[0],
        loss_functions=[loss_function],
        opt_state=opt_state,
        optimizer=optimizer,
    )

    # Save the results
    output_path = os.path.join(output_dir, f"{name}_results.hdf5")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    save_optimization_history_to_file(filename=output_path, history=optimizer.history)


def main(split_types_arg=None, n_steps=10000, num_splits=3):
    # define parameters
    ensembles = ["ISO_TRI", "ISO_BI"]
    losses = {"mcMSE": hdx_uptake_mean_centred_MSE_loss, "MSE": hdx_uptake_MSE_loss}
    # n_steps and num_splits now come from function arguments

    # Define convergence criteria from 1e-3 to 1e-10 as mentioned in docstring
    convergence_rates = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10]

    datasplit_dir = "_datasplits"
    datasplit_dir = os.path.join(os.path.dirname(__file__), datasplit_dir)
    if not os.path.exists(datasplit_dir):
        raise FileNotFoundError(f"Datasplit directory not found: {datasplit_dir}")

    features_dir = "_featurise"
    features_dir = os.path.join(os.path.dirname(__file__), features_dir)
    if not os.path.exists(features_dir):
        raise FileNotFoundError(f"Features directory not found: {features_dir}")

    # Load features - Fixed the path mapping
    feature_paths = {
        "ISO_TRI": os.path.join(features_dir, "features_iso_tri.npz"),
        "ISO_BI": os.path.join(features_dir, "features_iso_bi.npz"),
    }

    features = {
        key: (
            BV_input_features.load(path),
            pt.PTSerialiser.load_list_from_json(
                path.replace("features_", "topology_").replace(".npz", ".json")
            ),
        )
        for key, path in feature_paths.items()
    }

    # Setup BV model configuration
    bv_config = BV_model_Config(num_timepoints=5)
    bv_config.timepoints = jnp.array([0.167, 1.0, 10.0, 60.0, 120.0])
    bv_model = BV_model(config=bv_config)
    model_parameters = bv_model.params
    print(bv_model.config.key)  # Print the model key for debugging

    # Create base output directory
    output_base_dir = "_optimise"
    output_base_dir = os.path.join(os.path.dirname(__file__), output_base_dir)
    os.makedirs(output_base_dir, exist_ok=True)

    # Discover split types
    # Assuming split types are subdirectories within datasplit_dir
    split_types = [
        d for d in os.listdir(datasplit_dir) if os.path.isdir(os.path.join(datasplit_dir, d))
    ]
    if "full_dataset" in split_types:
        split_types.remove("full_dataset")  # Exclude the full_dataset directory from split types
    split_types.sort()  # Ensure consistent order

    # Filter split_types if argument provided
    if split_types_arg is not None:
        if split_types_arg == "all":
            pass  # Use all discovered split types
        else:
            requested = [s.strip() for s in split_types_arg.split(",")]
            split_types = [s for s in split_types if s in requested]
            if not split_types:
                raise ValueError(f"No valid split types selected from: {requested}")

    # Run optimization for all combinationsj
    total_runs = len(ensembles) * len(losses) * num_splits * len(split_types)
    current_run = 0

    print(f"Starting optimization with {total_runs} total runs...")
    print(f"Split Types: {split_types}")
    print(f"Ensembles: {ensembles}")
    print(f"Loss functions: {list(losses.keys())}")
    print(f"Number of splits per type: {num_splits}")
    print(f"Convergence rates: {convergence_rates}")
    print(f"Max steps per run: {n_steps}")
    print("-" * 60)

    total_start_time = time.time()  # Start total timer

    for split_type in split_types:
        print(f"\n-- Processing split type: {split_type} --")
        split_type_dir = os.path.join(datasplit_dir, split_type)
        output_dir = os.path.join(output_base_dir, split_type)
        os.makedirs(output_dir, exist_ok=True)

        # Load all data splits for this type
        splits = []
        try:
            for split_idx in range(num_splits):
                split_path = os.path.join(split_type_dir, f"split_{split_idx:03d}")

                train_data = HDX_peptide.load_list_from_files(
                    json_path=os.path.join(split_path, "train_topology.json"),
                    csv_path=os.path.join(split_path, "train_dfrac.csv"),
                )
                val_data = HDX_peptide.load_list_from_files(
                    json_path=os.path.join(split_path, "val_topology.json"),
                    csv_path=os.path.join(split_path, "val_dfrac.csv"),
                )
                splits.append((train_data, val_data))
        except FileNotFoundError as e:
            print(f"Could not load splits for {split_type}. Skipping. Error: {e}")
            continue

        for ensemble in ensembles:
            print(f"\nProcessing ensemble: {ensemble}")
            ensemble_features, ensemble_feature_top = features[ensemble]

            for loss_name, loss_function in losses.items():
                print(f"  Using loss function: {loss_name}")

                for split_idx, (train_data, val_data) in enumerate(splits):
                    current_run += 1

                    # Create descriptive name for this run
                    run_name = f"{ensemble}_{loss_name}_{split_type}_split{split_idx:03d}"

                    print(f"    Running split {split_idx} [{current_run}/{total_runs}]: {run_name}")
                    print(f"      Train samples: {len(train_data)}, Val samples: {len(val_data)}")

                    run_start_time = time.time()  # Start timer for this run

                    try:
                        run_optimise_ISO_TRI_BI(
                            train_data=train_data,
                            val_data=val_data,
                            features=ensemble_features,
                            forward_model=bv_model,
                            model_parameters=model_parameters,
                            feature_top=ensemble_feature_top,
                            convergence=convergence_rates,
                            loss_function=loss_function,
                            n_steps=n_steps,
                            name=run_name,
                            output_dir=output_dir,
                        )
                        run_elapsed = time.time() - run_start_time
                        print(f"      ✓ Completed: {run_name} (Elapsed: {run_elapsed:.2f} s)")

                    except Exception as e:
                        run_elapsed = time.time() - run_start_time
                        print(
                            f"      ✗ Failed: {run_name} - Error: {str(e)} (Elapsed: {run_elapsed:.2f} s)"
                        )
                        # Continue with other runs even if one fails
                        continue

    total_elapsed = time.time() - total_start_time  # End total timer

    print(f"\n{'=' * 60}")
    print("Optimization completed!")
    print(f"Results saved in: {output_base_dir}")
    print(f"Total runs attempted: {total_runs}")
    print(f"Total elapsed time: {total_elapsed:.2f} s")
    print("Check individual HDF5 files for detailed results.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Optimise ISO_TRI/BI splits.")
    parser.add_argument(
        "--split-types",
        type=str,
        default="all",
        help="Comma-separated list of split types to run (e.g. 'random,sequence'). Use 'all' for all types.",
    )
    parser.add_argument(
        "--n-steps",
        type=int,
        default=10000,
        help="Number of optimization steps per run (default: 10000).",
    )
    parser.add_argument(
        "--n-replicates",
        type=int,
        default=3,
        help="Number of replicates (splits) per split type (default: 3).",
    )
    args = parser.parse_args()
    main(split_types_arg=args.split_types, n_steps=args.n_steps, num_splits=args.n_replicates)
