"""
This script performs the fitting of the IsoValidation experiment.

This uses the features generated by the 'featurise_ISO_TRI_BI.py' script.
This then loads the data splits generated by the 'splitdata_ISO.py' script.


This script is used to fit:
- ISO-Bi modal and ISO-Tri modal ensenbles using just the hdx_uptake_mean_centred_MSE_loss.
- The fitting is performed over converagence rates 1e-3 to 1e-10 this is performed using the optimise_sweep function.
The data are saved using hdf5 format.

Usage:
python optimise_ISO_TRI_BI_splits.py --split-types all
python optimise_ISO_TRI_BI_splits.py --split-types random,sequence,sequence_cluster,stratified,spatial


"""

import argparse  # <-- Add this import
import os
import time  # <-- Add this import

import jax.numpy as jnp
from optimise_fn import run_optimise_ISO_TRI_BI

import jaxent.src.interfaces.topology as pt
from jaxent.src.custom_types.HDX import HDX_peptide
from jaxent.src.models.config import BV_model_Config
from jaxent.src.models.HDX.BV.features import BV_input_features
from jaxent.src.models.HDX.BV.forwardmodel import BV_model
from jaxent.src.opt.losses import (
    hdx_uptake_mean_centred_MSE_loss,
    hdx_uptake_MSE_loss,
)


def main(split_types_arg=None, n_steps=10000, num_splits=3):
    # define parameters
    ensembles = ["ISO_TRI", "ISO_BI"]
    losses = {"mcMSE": hdx_uptake_mean_centred_MSE_loss, "MSE": hdx_uptake_MSE_loss}
    # n_steps and num_splits now come from function arguments

    # Define convergence criteria from 1e-3 to 1e-10 as mentioned in docstring
    convergence_rates = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10]

    datasplit_dir = "_datasplits"
    datasplit_dir = os.path.join(os.path.dirname(__file__), datasplit_dir)
    if not os.path.exists(datasplit_dir):
        raise FileNotFoundError(f"Datasplit directory not found: {datasplit_dir}")

    features_dir = "_featurise"
    features_dir = os.path.join(os.path.dirname(__file__), features_dir)
    if not os.path.exists(features_dir):
        raise FileNotFoundError(f"Features directory not found: {features_dir}")

    # Load features - Fixed the path mapping
    feature_paths = {
        "ISO_TRI": os.path.join(features_dir, "features_iso_tri.npz"),
        "ISO_BI": os.path.join(features_dir, "features_iso_bi.npz"),
    }

    features = {
        key: (
            BV_input_features.load(path),
            pt.PTSerialiser.load_list_from_json(
                path.replace("features_", "topology_").replace(".npz", ".json")
            ),
        )
        for key, path in feature_paths.items()
    }

    # Setup BV model configuration
    bv_config = BV_model_Config(num_timepoints=5)
    bv_config.timepoints = jnp.array([0.167, 1.0, 10.0, 60.0, 120.0])
    bv_model = BV_model(config=bv_config)
    model_parameters = bv_model.params
    print(bv_model.config.key)  # Print the model key for debugging

    # Create base output directory
    output_base_dir = "_optimise"
    output_base_dir = os.path.join(os.path.dirname(__file__), output_base_dir)
    os.makedirs(output_base_dir, exist_ok=True)

    # Discover split types
    # Assuming split types are subdirectories within datasplit_dir
    split_types = [
        d for d in os.listdir(datasplit_dir) if os.path.isdir(os.path.join(datasplit_dir, d))
    ]
    if "full_dataset" in split_types:
        split_types.remove("full_dataset")  # Exclude the full_dataset directory from split types
    split_types.sort()  # Ensure consistent order

    # Filter split_types if argument provided
    if split_types_arg is not None:
        if split_types_arg == "all":
            pass  # Use all discovered split types
        else:
            requested = [s.strip() for s in split_types_arg.split(",")]
            split_types = [s for s in split_types if s in requested]
            if not split_types:
                raise ValueError(f"No valid split types selected from: {requested}")

    # Run optimization for all combinationsj
    total_runs = len(ensembles) * len(losses) * num_splits * len(split_types)
    current_run = 0

    print(f"Starting optimization with {total_runs} total runs...")
    print(f"Split Types: {split_types}")
    print(f"Ensembles: {ensembles}")
    print(f"Loss functions: {list(losses.keys())}")
    print(f"Number of splits per type: {num_splits}")
    print(f"Convergence rates: {convergence_rates}")
    print(f"Max steps per run: {n_steps}")
    print("-" * 60)

    total_start_time = time.time()  # Start total timer

    for split_type in split_types:
        print(f"\n-- Processing split type: {split_type} --")
        split_type_dir = os.path.join(datasplit_dir, split_type)
        output_dir = os.path.join(output_base_dir, split_type)
        os.makedirs(output_dir, exist_ok=True)

        # Load all data splits for this type
        splits = []
        try:
            for split_idx in range(num_splits):
                split_path = os.path.join(split_type_dir, f"split_{split_idx:03d}")

                train_data = HDX_peptide.load_list_from_files(
                    json_path=os.path.join(split_path, "train_topology.json"),
                    csv_path=os.path.join(split_path, "train_dfrac.csv"),
                )
                val_data = HDX_peptide.load_list_from_files(
                    json_path=os.path.join(split_path, "val_topology.json"),
                    csv_path=os.path.join(split_path, "val_dfrac.csv"),
                )
                splits.append((train_data, val_data))
        except FileNotFoundError as e:
            print(f"Could not load splits for {split_type}. Skipping. Error: {e}")
            continue

        for ensemble in ensembles:
            print(f"\nProcessing ensemble: {ensemble}")
            ensemble_features, ensemble_feature_top = features[ensemble]

            for loss_name, loss_function in losses.items():
                print(f"  Using loss function: {loss_name}")

                for split_idx, (train_data, val_data) in enumerate(splits):
                    current_run += 1

                    # Create descriptive name for this run
                    run_name = f"{ensemble}_{loss_name}_{split_type}_split{split_idx:03d}"

                    print(f"    Running split {split_idx} [{current_run}/{total_runs}]: {run_name}")
                    print(f"      Train samples: {len(train_data)}, Val samples: {len(val_data)}")

                    run_start_time = time.time()  # Start timer for this run

                    try:
                        run_optimise_ISO_TRI_BI(
                            train_data=train_data,
                            val_data=val_data,
                            features=ensemble_features,
                            forward_model=bv_model,
                            model_parameters=model_parameters,
                            feature_top=ensemble_feature_top,
                            convergence=convergence_rates,
                            loss_function=loss_function,
                            n_steps=n_steps,
                            name=run_name,
                            output_dir=output_dir,
                        )
                        run_elapsed = time.time() - run_start_time
                        print(f"      ✓ Completed: {run_name} (Elapsed: {run_elapsed:.2f} s)")

                    except Exception as e:
                        run_elapsed = time.time() - run_start_time
                        print(
                            f"      ✗ Failed: {run_name} - Error: {str(e)} (Elapsed: {run_elapsed:.2f} s)"
                        )
                        # Continue with other runs even if one fails
                        continue

    total_elapsed = time.time() - total_start_time  # End total timer

    print(f"\n{'=' * 60}")
    print("Optimization completed!")
    print(f"Results saved in: {output_base_dir}")
    print(f"Total runs attempted: {total_runs}")
    print(f"Total elapsed time: {total_elapsed:.2f} s")
    print("Check individual HDF5 files for detailed results.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Optimise ISO_TRI/BI splits.")
    parser.add_argument(
        "--split-types",
        type=str,
        default="all",
        help="Comma-separated list of split types to run (e.g. 'random,sequence'). Use 'all' for all types.",
    )
    parser.add_argument(
        "--n-steps",
        type=int,
        default=10000,
        help="Number of optimization steps per run (default: 10000).",
    )
    parser.add_argument(
        "--n-replicates",
        type=int,
        default=3,
        help="Number of replicates (splits) per split type (default: 3).",
    )
    args = parser.parse_args()
    main(split_types_arg=args.split_types, n_steps=args.n_steps, num_splits=args.n_replicates)
